{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of giving arbitory information of RDD. We give those row some structure and treate RDD as DATABASES.\n",
    "and its called \"DATAFRAME\" and \"DATASETS\".\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing.\n",
    "the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed.There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques.\n",
    "\n",
    "A DataFrame can be constructed from  different sources such as Hive tables, Structured Data files, external databases, or existing RDDs. This API was designed for modern Big Data and data science applications.\n",
    "\n",
    "Data in Dataframe is store in form of Columns and Row format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATAFRAME BASICS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To start with DATAFRAME First we have to start \"SPARK SESSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLContext is a class and is used for initializing the functionalities of Spark SQL. SparkContext class object (sc) is required for initializing SQLContext class object.\n",
    "\n",
    "The following command is used for initializing the SparkContext through spark-shell.\n",
    "\n",
    "$ spark-shell\n",
    "\n",
    "By default, the SparkContext object is initialized with the name sc when the spark-shell starts.\n",
    "\n",
    "Use the following command to create SQLContext.\n",
    "\n",
    "scala> val sqlcontext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we treate RDD as ROW OBJECT which contain columns.\n",
    "\n",
    "2. we can perform \"SQL\" like QUERY on it.\n",
    "\n",
    "3. It has SCHEMA. i.e STRUCTURE. \n",
    "\n",
    "4. We can Read and Write to JSON, HIVE\n",
    "\n",
    "5. communicate with JDBC/ODBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have input data is  structure data like data store in relational database then we can use it in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We can create DATAFRAME using :\n",
    "           1. Import Structure data from JDBC/ODBC\n",
    "           2. Import JSON Data\n",
    "           3. Convert RDD to DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  But once we have DATAFRAME :\n",
    "                we can do \"CreateorReplaceTempView\" to actual create table in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Operations That can perform on DATAFRAME:\n",
    "        1. myresultDataFrame.show()                               #to show data in dataframe\n",
    "        2. myresultDataFrame.select(\"id\")                         #to select id column from dataframe\n",
    "        3. myresultDataFrame.filter(myresultDataFrame(\"id\" > 5))        \n",
    "        4. myresultDataFrame.groupBy(myresultDataFrame(id)).mean()\n",
    "        5. myresultDataFrame.rdd().map(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In SPARK 2.0 \"DataFrame\"  is \"DATASET\"  of row object. Dataset is Dataframe of Structure DATA. \n",
    "### Dataframe isn't Dataset but Dataset is DataFrame of Structure Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can define our own user defined function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIFFERENCE BETWEEN RDD, DATAFRAME AND DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Apache Spark APIs – RDD, DataFrame, and DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Before starting the comparison between Spark RDD vs DataFrame vs Dataset, let us see RDDs, DataFrame and Datasets in Spark:\n",
    "\n",
    "###### 1. Spark RDD APIs –\n",
    "                An RDD stands for Resilient Distributed Datasets. It is Read-only partition collection of records. RDD is the fundamental data structure of Spark. It allows a programmer to perform in-memory computations on large clusters in a fault-tolerant manner. Thus, speed up the task. Follow this link to learn Spark RDD in great detail.\n",
    "###### 2. Spark Dataframe APIs – \n",
    "                Unlike an RDD, data organized into named columns. For example a table in a relational database. It is an immutable distributed collection of data. DataFrame in Spark allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction. Follow this link to learn Spark DataFrame in detail.\n",
    "###### 3. Spark Dataset APIs –\n",
    "                Datasets in Apache Spark are an extension of DataFrame API which provides type-safe, object-oriented programming interface. Dataset takes advantage of Spark’s Catalyst optimizer by exposing expressions and data fields to a query planner. Follow this link to learn Spark DataSet in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation in RDD, DataFrame, and DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RDD – \n",
    "        RDD is a distributed collection of data elements spread across many machines in the cluster. RDDs are a set of Java or Scala objects representing data.\n",
    "###### DataFrame – \n",
    "            A DataFrame is a distributed collection of data organized into named columns. It is conceptually equal to a table in a relational database.\n",
    "###### DataSet –\n",
    "        It is an extension of DataFrame API that provides the functionality of – type-safe, object-oriented programming interface of the RDD API and performance benefits of the Catalyst query optimizer and off heap storage mechanism of a DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Formats in RDD, DataFrame, and DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RDD – \n",
    "        It can easily and efficiently process data which is structured as well as unstructured. But like Dataframe and DataSets, RDD does not infer the schema of the ingested data and requires the user to specify it.\n",
    "###### DataFrame –\n",
    "            It can process structured and unstructured data efficiently. It organizes the data in the named column. DataFrames allow the Spark to manage schema.\n",
    "###### DataSet –\n",
    "            It also efficiently processes structured and unstructured data. It represents data in the form of JVM objects of row or a collection of row object. Which is represented in tabular forms through encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset (RDD)\n",
    "\n",
    "RDD was the primary user-facing API in Spark since its inception. At the core, an RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers transformations and actions.\n",
    "\n",
    "#### When to use RDDs?\n",
    "\n",
    "Consider these scenarios or common use cases for using RDDs when:\n",
    "\n",
    "you want low-level transformation and actions and control on your dataset;\n",
    "your data is unstructured, such as media streams or streams of text;\n",
    "you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column; and\n",
    "you can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.\n",
    "\n",
    "\n",
    "##  DataFrames\n",
    "\n",
    "Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier, DataFrame allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction; it provides a domain specific language API to manipulate your distributed data; and makes Spark accessible to a wider audience, beyond specialized data engineers.\n",
    "\n",
    "In our preview of Apache Spark 2.0 webinar and subsequent blog, we mentioned that in Spark 2.0, DataFrame APIs will merge with Datasets APIs, unifying data processing capabilities across libraries. Because of this unification, developers now have fewer concepts to learn or remember, and work with a single high-level and type-safe API called Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC EXAMPLE OF SPARK DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f5ebb267950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, desc\n",
    "\n",
    "spark = SparkSession.builder.appName('airflight').getOrCreate()\n",
    "\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), age=int(fields[2]), numFriends=int(fields[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"file:///home/icpl12900/Desktop/assignments/fakefriends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,Will,33,385',\n",
       " u'1,Jean-Luc,26,2',\n",
       " u'2,Hugh,55,221',\n",
       " u'3,Deanna,40,465',\n",
       " u'4,Quark,68,21',\n",
       " u'5,Weyoun,59,318',\n",
       " u'6,Gowron,37,220',\n",
       " u'7,Will,54,307',\n",
       " u'8,Jadzia,38,380',\n",
       " u'9,Hugh,27,181']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people = lines.map(mapper) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=0, age=33, name='Will', numFriends=385)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaPeople.createOrReplaceTempView(\"people\")  \n",
    "#here, we create temporary table from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ID=21, age=19, name=u'Miles', numFriends=268)\n",
      "Row(ID=52, age=19, name=u'Beverly', numFriends=269)\n",
      "Row(ID=54, age=19, name=u'Brunt', numFriends=5)\n",
      "Row(ID=106, age=18, name=u'Beverly', numFriends=499)\n",
      "Row(ID=115, age=18, name=u'Dukat', numFriends=397)\n",
      "Row(ID=133, age=19, name=u'Quark', numFriends=265)\n",
      "Row(ID=136, age=19, name=u'Will', numFriends=335)\n",
      "Row(ID=225, age=19, name=u'Elim', numFriends=106)\n",
      "Row(ID=304, age=19, name=u'Will', numFriends=404)\n",
      "Row(ID=341, age=18, name=u'Data', numFriends=326)\n",
      "Row(ID=366, age=19, name=u'Keiko', numFriends=119)\n",
      "Row(ID=373, age=19, name=u'Quark', numFriends=272)\n",
      "Row(ID=377, age=18, name=u'Beverly', numFriends=418)\n",
      "Row(ID=404, age=18, name=u'Kasidy', numFriends=24)\n",
      "Row(ID=409, age=19, name=u'Nog', numFriends=267)\n",
      "Row(ID=439, age=18, name=u'Data', numFriends=417)\n",
      "Row(ID=444, age=18, name=u'Keiko', numFriends=472)\n",
      "Row(ID=492, age=19, name=u'Dukat', numFriends=36)\n",
      "Row(ID=494, age=18, name=u'Kasidy', numFriends=194)\n"
     ]
    }
   ],
   "source": [
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "for teen in teenagers.collect():\n",
    "  print(teen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 18|    8|\n",
      "| 19|   11|\n",
      "| 20|    5|\n",
      "| 21|    8|\n",
      "| 22|    7|\n",
      "| 23|   10|\n",
      "| 24|    5|\n",
      "| 25|   11|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 28|   10|\n",
      "| 29|   12|\n",
      "| 30|   11|\n",
      "| 31|    8|\n",
      "| 32|   11|\n",
      "| 33|   12|\n",
      "| 34|    6|\n",
      "| 35|    8|\n",
      "| 36|   10|\n",
      "| 37|    9|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POPULAR MOVIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath =sc.textFile(\"file:///home/icpl12900/Desktop/assignments/ml-100k/u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'196\\t242\\t3\\t881250949', u'186\\t302\\t3\\t891717742']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///home/icpl12900/Desktop/assignments/ml-100k/u.data MapPartitionsRDD[34] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split('\\t')\n",
    "    return Row(ID=int(fields[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert it to a RDD of Row objects\n",
    "data = filepath.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert that to a DataFrame\n",
    "movieDataset = spark.createDataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popularMovie = movieDataset.groupby(\"ID\").count().orderBy(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=405, count=737), Row(ID=655, count=685)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularMovie.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
